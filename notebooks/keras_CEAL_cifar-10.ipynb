{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cost%20Effective%20Active%20Learning%20for%20Deep%20Image%20Classification.png](attachment:Cost%20Effective%20Active%20Learning%20for%20Deep%20Image%20Classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 2.6408 - acc: 0.2196 - val_loss: 14.6165 - val_acc: 0.0872\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 2.1483 - acc: 0.2929 - val_loss: 14.7511 - val_acc: 0.0769\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 2.0571 - acc: 0.3127 - val_loss: 14.6288 - val_acc: 0.0798\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 2.0258 - acc: 0.3158 - val_loss: 14.7057 - val_acc: 0.0831\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 2.0217 - acc: 0.3169 - val_loss: 14.6324 - val_acc: 0.0832\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications import mobilenet, nasnet\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, ProgbarLogger\n",
    "\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling2D, Reshape, Conv2D\n",
    "\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "# from keras.utils import plot_model\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "initial_train_perc = 0.2\n",
    "initial_train_size = int(x_train.shape[0] * initial_train_perc)\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "x_train_initial, y_train_initial = iter(datagen.flow(x_train, y_train, batch_size=initial_train_size, shuffle=True)).next()\n",
    "\n",
    "print(x_train_initial.shape)\n",
    "print(y_train_initial.shape)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "input_shape = x_train[-1,].shape\n",
    "# input_tensor = Input(shape=input_shape)\n",
    "\n",
    "def _nasnet(num_classes, input_shape=(32,32,3), pretrained=True, freezed=True):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    weights = 'imagenet' if pretrained else None\n",
    "    base_model = nasnet.NASNetMobile(input_tensor=input_tensor, weights=weights, include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    if freezed:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def _mobilenet(num_classes, input_shape=(32,32,3), pretrained=True, freezed=True):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    weights = 'imagenet' if pretrained else None\n",
    "    base_model = mobilenet.MobileNet(input_tensor=input_tensor, weights=weights, include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape(shape, name='reshape_1')(x)\n",
    "    x = Dropout(dropout, name='dropout')(x)\n",
    "    x = Conv2D(classes, (1, 1), padding='same', name='conv_preds')(x)\n",
    "    x = Activation('softmax', name='act_softmax')(x)\n",
    "    predictions = Reshape((num_classes,), name='reshape_2')(x)\n",
    "    if freezed:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "        \n",
    "model = _nasnet(num_classes, input_shape)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "# model = load_model('ceal_initial_nasnet.hdf5')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# checkpointer = ModelCheckpoint(filepath='ceal_initial_nasnet.hdf5', verbose=1, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "# tensorboard = TensorBoard()\n",
    "# progbar = ProgbarLogger()\n",
    "# earlystop = EarlyStopping(patience=5)\n",
    "\n",
    "hist = model.fit(x_train_initial, y_train_initial, validation_data=(x_test, y_test), epochs=5)\n",
    "#                  callbacks=[checkpointer, reduce_lr, tensorboard, progbar, earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "Not using data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 102s 2ms/step - loss: 1.7161 - acc: 0.5075 - val_loss: 1.4815 - val_acc: 0.5608\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1.3180 - acc: 0.6297 - val_loss: 1.2774 - val_acc: 0.6414\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1.1413 - acc: 0.6853 - val_loss: 1.1574 - val_acc: 0.6750\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1.0409 - acc: 0.7203 - val_loss: 1.0843 - val_acc: 0.7039\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.9568 - acc: 0.7485 - val_loss: 1.0565 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.8839 - acc: 0.7750 - val_loss: 1.0374 - val_acc: 0.7172\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.8296 - acc: 0.7935 - val_loss: 1.0089 - val_acc: 0.7369\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.7825 - acc: 0.8150 - val_loss: 1.0429 - val_acc: 0.7304\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.7414 - acc: 0.8302 - val_loss: 1.1550 - val_acc: 0.7028\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.7105 - acc: 0.8438 - val_loss: 1.0402 - val_acc: 0.7395\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.6827 - acc: 0.8549 - val_loss: 1.0799 - val_acc: 0.7355\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.6591 - acc: 0.8670 - val_loss: 1.0712 - val_acc: 0.7415\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.6365 - acc: 0.8751 - val_loss: 1.1226 - val_acc: 0.7372\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.4447 - acc: 0.9450 - val_loss: 1.1190 - val_acc: 0.7666\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.3740 - acc: 0.9639 - val_loss: 1.2017 - val_acc: 0.7607\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.3444 - acc: 0.9707 - val_loss: 1.2690 - val_acc: 0.7623\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 0.3277 - acc: 0.9734 - val_loss: 1.3388 - val_acc: 0.7590\n",
      "10000/10000 [==============================] - 4s 374us/step\n",
      "Test loss :  1.338844920539856\n",
      "Test accuracy :  0.759\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from keras example cifar10_cnn.py and github.com/raghakot/keras-resnet\n",
    "Train ResNet-18 on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_contrib.applications.resnet import ResNet18\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "weights_file = 'ResNet18v2-CIFAR-10.h5'\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('ResNet18v2-CIFAR-10.csv')\n",
    "model_checkpoint = ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True,\n",
    "                                   save_weights_only=True, mode='auto')\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 3\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_train /= 128.\n",
    "X_test /= 128.\n",
    "\n",
    "model = ResNet18((img_rows, img_cols, img_channels), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          shuffle=True,\n",
    "          callbacks=[lr_reducer, early_stopper, csv_logger, model_checkpoint])\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print('Test loss : ', scores[0])\n",
    "print('Test accuracy : ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in log\n",
      "/home/daniel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 32, 32, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d11aae80c821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m#     print(y_train_initial.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mal_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mevaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import scipy as sc\n",
    "import heapq\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "def least_confidence(y_pred_prob, y_true):\n",
    "    \n",
    "    origin_index = np.arange(0,len(y_pred_prob))\n",
    "    max_prob = np.max(y_pred_prob, axis=1)\n",
    "    max_prob_index = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    lci = np.column_stack((origin_index,\n",
    "                            max_prob,\n",
    "                            max_prob_index, \n",
    "                            y_true))\n",
    "    \n",
    "    lci = lci[lci[:,1].argsort()]\n",
    "    return lci, lci[:,0].astype(int)\n",
    "\n",
    "def margin_sampling(y_pred_prob, y_true):\n",
    "    \n",
    "    origin_index = np.arange(0,len(y_pred_prob))\n",
    "    max_prob = np.max(y_pred_prob, axis=1)\n",
    "    max_prob_index = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    for row in y_pred_prob:\n",
    "#         a = heapq.nlargest(2, range(len(row)), row.take)\n",
    "        a = row.argsort()[-2:][::-1]\n",
    "        b = np.take(row, a)\n",
    "#   \n",
    "    return np.sort(np.amax(y_pred_prob, axis=1))\n",
    "\n",
    "def entropy(y_pred_prob, y_true):\n",
    "#     entropy = sc.stats.entropy(y_pred_prob, base=2, axis=1)\n",
    "#     entropy = np.nan_to_num(entropy)\n",
    "    origin_index = np.arange(0,len(y_pred_prob))\n",
    "    max_prob = np.max(y_pred_prob, axis=1)\n",
    "    max_prob_index = np.argmax(y_pred_prob, axis=1)\n",
    "    entropy = -np.nansum(np.multiply(y_pred_prob, np.log(y_pred_prob)), axis=1)\n",
    "    eni = np.column_stack((origin_index,\n",
    "                            entropy,\n",
    "                            max_prob,\n",
    "                            max_prob_index, \n",
    "                            y_true))\n",
    "                           \n",
    "    eni = eni[(-eni[:,1]).argsort()]\n",
    "    return eni, eni[:,0].astype(int)\n",
    "\n",
    "def high_confidence(y_pred_prob, y_true, delta):\n",
    "    eni, eni_idx = entropy(y_pred_prob, y_true)\n",
    "    hcs = eni[eni[:,1] < delta]\n",
    "    return hcs, hcs[:,0].astype(int)\n",
    "\n",
    "\n",
    "##### CEAL parameters #####\n",
    "\n",
    "#maximum iteration numbers\n",
    "T=50\n",
    "#fine-tuning interval\n",
    "t=1\n",
    "#threshold decay rate\n",
    "dr= 0.00033\n",
    "#high confidence samples selection threshold\n",
    "delta=0.005\n",
    "#uncertain samples selection size\n",
    "K=1000\n",
    "\n",
    "#unlabeled samples\n",
    "DU = None\n",
    "#initially labeled samples\n",
    "DL = None\n",
    "#high confidence samples\n",
    "DH = None\n",
    "\n",
    "dataset_size = 1000\n",
    "num_classes = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "x = x_train#[0:dataset_size]\n",
    "y = y_train#[0:dataset_size]\n",
    "\n",
    "# y_pred_prob = np.random.rand((y.shape[0]), num_classes)\n",
    "# y_pred_prob = y_pred_prob / y_pred_prob.sum(axis=1, keepdims=True)\n",
    "# y_pred_prob = np.around(y_pred_prob, 3)\n",
    "# print(y_pred_prob)\n",
    "# print(np.sum(y_pred_prob))\n",
    "\n",
    "for i in range(T):  \n",
    "    y_pred_prob = model.predict(x, verbose=0)\n",
    "#     y_pred_prob = np.around(y_pred_prob, 3)\n",
    "\n",
    "    isa, isa_idx = least_confidence(y_pred_prob, y)\n",
    "    # isa, isa_idx = margin_sampling(y_pred_prod, y)\n",
    "    # isa, isa_idx = entropy(y_pred_prob, y)\n",
    "    hcs, hcs_idx = high_confidence(y_pred_prob, y, delta)\n",
    "    \n",
    "    idx_concat = np.concatenate((isa_idx, hcs_idx),0)\n",
    "    idx = np.unique(idx_concat, return_index=True)[1]\n",
    "    idx = np.array([idx_concat[i] for i in sorted(idx)])\n",
    "    \n",
    "    step = i*K\n",
    "    DH = np.take(x, idx[step+K:], axis=0), np.take(y, idx[step+K:], axis=0)\n",
    "    DL = np.take(x, idx[step:step+K], axis=0), np.take(y, idx[step:step+K], axis=0)\n",
    "    x = np.delete(x, idx[step:step+K], axis=0)\n",
    "    y = np.delete(y, idx[step:step+K], axis=0)\n",
    "    print(x.shape)\n",
    "    \n",
    "#     x_uncertain = np.take(x, isa_idx[step:step+K], axis=0)\n",
    "#     y_uncertain = np.take(y, isa_idx[step:step+K], axis=0)\n",
    "    \n",
    "#     if DL is None:\n",
    "#         DL = x_uncertain, y_uncertain\n",
    "#     else:\n",
    "#         x_l, y_l = DL\n",
    "#         x_l = np.append(x_l, x_uncertain, axis=0)\n",
    "#         y_l = np.append(y_l, y_uncertain, axis=0)\n",
    "#         DL = x_l, y_l\n",
    "        \n",
    "    al_x, al_y = np.append(DL[0], DH[0], axis=0), np.append(DL[1], DH[1], axis=0)\n",
    "#     print(DL[0].shape)\n",
    "#     print(DL[1].shape)\n",
    "#     print(DH[0].shape)\n",
    "#     print(DH[1].shape)\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     print(al_x.shape)\n",
    "#     print(al_y.shape)\n",
    "#     print(x_train_initial.shape)\n",
    "#     print(y_train_initial.shape)\n",
    "    if i % t == 0:\n",
    "        model.fit(al_x, al_y, epochs=5, verbose=0)\n",
    "        delta -= (dr * (i//t))\n",
    "    evaluate = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print(evaluate)\n",
    "\n",
    "\n",
    "# print(classification_report(np.argmax(y, axis=1), y_pred_idx))\n",
    "# print(confusion_matrix(np.argmax(y, axis=1), y_pred_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
